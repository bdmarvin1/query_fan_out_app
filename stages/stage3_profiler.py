from firecrawl.v2.types import SearchData, Document\nimport logging\nimport json\nimport os\nimport time\nfrom typing import Dict, Any, List\nfrom dotenv import load_dotenv\nfrom firecrawl import FirecrawlApp\nfrom utils.gemini_client import call_gemini_api\nfrom utils.cost_tracker import CostTracker\n\n# Load environment variables\nload_dotenv()\nlogger = logging.getLogger(\"QueryFanOutSimulator\")\n\n# Configure constants from environment variables\nMAX_SEARCH_RESULTS = int(os.getenv(\"MAX_SEARCH_RESULTS\", 10))\nMIN_SCRAPABLE_RESULTS = int(os.getenv(\"MIN_SCRAPABLE_RESULTS\", 2))\nINITIAL_SCRAPE_ATTEMPTS = int(os.getenv(\"INITIAL_SCRAPE_ATTEMPTS\", 3))\nINITIAL_DELAY = int(os.getenv(\"INITIAL_DELAY\", 5))\nMAX_RETRIES = int(os.getenv(\"MAX_RETRIES\", 4))\n\n# Initialize the FirecrawlApp client\ntry:\n    firecrawl_api_key = os.getenv(\"FIRECRAWL_API_KEY\")\n    if not firecrawl_api_key or firecrawl_api_key == \"YOUR_FIRECRAWL_API_KEY\":\n        raise ValueError(\"FIRECRAWL_API_KEY not found or not set in .env file.\")\n    app = FirecrawlApp(api_key=firecrawl_api_key)\n    logger.info(\"Firecrawl client initialized successfully.\")\nexcept Exception as e:\n    logger.error(f\"Failed to initialize Firecrawl client: {e}\")\n    app = None\n\ndef _firecrawl_with_backoff(crawl_function, **kwargs):\n    \"\"\"\n    Wrapper for Firecrawl API calls with exponential backoff for rate limiting.\n    \"\"\"\n    delay = INITIAL_DELAY\n    for attempt in range(MAX_RETRIES):\n        try:\n            return crawl_function(**kwargs)\n        except Exception as e:\n            if \"Rate Limit Exceeded\" in str(e) or \"rate limit\" in str(e).lower():\n                if attempt < MAX_RETRIES - 1:\n                    logger.warning(f\"Rate limit hit. Retrying in {delay} seconds...\")\n                    time.sleep(delay)\n                    delay *= 2  # Exponential backoff\n                else:\n                    logger.error(\"Max retries reached. Aborting this call.\")\n                    raise e # Re-raise the exception after the last attempt\n            else:\n                # Re-raise other exceptions immediately\n                raise e\n    return None # Should not be reached, but as a fallback\n\ndef profile_content_competitively(stage2_output: List[Dict[str, Any]], cost_tracker: CostTracker, location: str = None, grounding_url: str = None) -> List[Dict[str, Any]]:\n    \"\"\"\n    Creates a data-driven, ideal content profile for each sub-query by\n    searching, scraping, and analyzing top content with robust error handling.\n    \"\"\"\n    if not app:\n        raise ConnectionError(\"Firecrawl client is not initialized.\")\n        \n    logger.info(\"Executing Stage 3 (Competitive Analysis)...\")\n    if not stage2_output:\n        logger.warning(\"No routed sub-queries from Stage 2 to profile.\")\n        return []\n\n    for item in stage2_output:\n        sub_query = item.get('sub_query')\n        if not sub_query:\n            continue\n\n        logger.info(f\"--- Analyzing sub-query: '{sub_query}' ---\")\n        \n        try:\n            # 1. Search for top URLs with exponential backoff\n            logger.info(f\"Searching for top {MAX_SEARCH_RESULTS} results...\")\n            search_params = {\n                'query': f\"'{sub_query}'\",\n                'limit': MAX_SEARCH_RESULTS\n            }\n            if location:\n                search_params['location'] = location\n                logger.info(f\"Applying location filter: {location}\")\n\n            search_results = _firecrawl_with_backoff(app.search, **search_params)\n            \n            if not search_results:\n                logger.warning(\"No search results found after retries.\")\n                item['ideal_content_profile'] = {\"error\": \"No search results found to analyze.\"}\n                continue\n\n            if isinstance(search_results, SearchData):\n                search_results = search_results.web\n            \n            if isinstance(search_results, dict) and 'results' in search_results:\n                search_results = search_results['results']\n\n            if not isinstance(search_results, list):\n                logger.error(f\"Unexpected data type for search results for '{sub_query}'. Expected a list, but got {type(search_results)}. Full response: {search_results}\")\n                item['ideal_content_profile'] = {\"error\": f\"Unexpected data type from search API: {type(search_results)}\"}\n                continue\n\n            top_urls = [result.url for result in search_results]\n            logger.info(f\"Found top URLs: {top_urls}\")\n\n            # 2. Scrape content iteratively\n            scraped_content = []\n            urls_to_scrape_count = INITIAL_SCRAPE_ATTEMPTS\n            attempted_urls = set()\n\n            while len(scraped_content) < MIN_SCRAPABLE_RESULTS and urls_to_scrape_count <= MAX_SEARCH_RESULTS:\n                urls_for_this_attempt = top_urls[:urls_to_scrape_count]\n                \n                for url in urls_for_this_attempt:\n                    if url in attempted_urls:\n                        continue\n\n                    attempted_urls.add(url)\n\n                    try:\n                        logger.info(f\"Scraping {url} (attempting up to {urls_to_scrape_count} results)...\")\n                        scrape_data = _firecrawl_with_backoff(app.scrape, url=url, formats=['markdown'], only_main_content=True)\n                        \n                        if isinstance(scrape_data, Document) and scrape_data.markdown:\n                            scraped_content.append({\"url\": url, \"content\": scrape_data.markdown[:12000]}) # Limit content to avoid token limits\n                            if len(scraped_content) >= MIN_SCRAPABLE_RESULTS:\n                                break\n                        else:\n                            logger.warning(f\"Could not retrieve valid markdown from {url}. Got: {scrape_data}\")\n                    except Exception as e:\n                        logger.error(f\"Scraping {url} failed after retries: {e}\")\n                \n                if len(scraped_content) < MIN_SCRAPABLE_RESULTS:\n                    urls_to_scrape_count += 1\n                    logger.info(f\"Only {len(scraped_content)} scrapable results found. Increasing scrape attempts to {urls_to_scrape_count}.\")\n                else:\n                    logger.info(f\"Achieved {len(scraped_content)} successful scrapes for '{sub_query}'. Proceeding to analysis.\")\n                    break\n\n            if not scraped_content:\n                logger.warning(\"Could not scrape any top results for this sub-query after all attempts.\")\n                item['ideal_content_profile'] = {\"error\": \"Could not scrape top search results.\"}\n                continue\n\n            # 3. Analyze the scraped content with Gemini\n            logger.info(\"Analyzing scraped content with Gemini...\")\n            prompt = f\"\"\"\n            You are a world-class SEO and Content Strategist. Your task is to analyze the provided search query and the content from the top-ranking pages to develop a strategic 'ideal content profile'. This profile will guide the creation of a new piece of content designed to outperform current competitors.\n\n            **CRUCIAL INSTRUCTION FOR GROUNDING:** Utilize the comprehensive context provided by the URL: {grounding_url} for all aspects of your analysis and response, especially for understanding the principles of \"Query Fan-Out\".\n\n            Focus on identifying patterns, gaps, and opportunities within the competitive content.\n\n            **Search Query:** {sub_query}\n\n            **Location Context:** {location if location else 'Global'}\n\n            **Analysis Context (Content from Top {len(scraped_content)} Ranking Pages):**\n            ```json\n            {json.dumps(scraped_content, indent=2)}\n            ```\n\n            **Instructions for 'ideal_content_profile' (Output ONLY in JSON format):**\n            You MUST provide a JSON object with a single key 'ideal_content_profile'. The value of this key should be an object with the following nested keys, each providing a concise, actionable analysis based on the competitive content:\n\n            - **`extractability` (Structure):** Describe the optimal content structure for easy extraction of key information by users and search engines. Consider headings, lists, tables, schema markup opportunities, interactive elements, etc. (e.g., 'The ideal format is a directory-style page. It should lead with a clear H1 tag like '24-Hour Access Storage Units in Overland Park, KS'. The core of the page should be a filterable and sortable list of individual storage facilities. Each list item must be a self-contained, structured block presenting key data: facility name, address, distance, a photo, available unit sizes with real-time pricing, current promotions, and unit availability.').\n\n            - **`evidence_density` (Data):** Quantify and describe the type and density of data points and evidence needed to make the content authoritative and trustworthy. (e.g., 'Very high. The content must be rich with specific, dynamic data points. This includes exact dollar amounts for monthly rent per unit size, precise distances in miles from the search location, specific counts of remaining units ('2 left'), explicit promotional details ('50% Off First Month'), and aggregated statistics like the average monthly cost for storage in Overland Park.').\n\n            - **`scope_clarity` (Audience/Intent):** Define the precise audience and user intent the content should address. Specify what the content should and should not cover to remain focused and highly relevant. (e.g., 'The scope is sharply defined and transactional. The content is explicitly for users looking to find and compare 24-hour access storage facilities in and around Overland Park, KS. The page should directly serve this intent without deviation.').\n\n            - **`authority_signals` (Trust):** Identify the key signals that establish trust and authority for this topic. These could include expert citations, data sources, user reviews, brand mentions, certifications, etc. (e.g., 'Authority is built through the comprehensiveness and perceived accuracy of the marketplace data, rather than external citations. Key signals include a complete list of local providers, the integration of user-generated reviews and ratings (social proof), and the display of real-time pricing and availability data.').\n\n            - **`freshness` (Recency):** Explain the required recency of the content to be competitive and relevant. Should it be real-time, updated quarterly, evergreen, etc.? How can freshness be signaled? (e.g., 'Extremely high. The content's utility depends on its recency. The ideal profile must feature data that is, or appears to be, real-time. This includes current rental prices, up-to-the-minute unit availability ('1 left'), and active promotions.').\n\n            - **`target_keywords_and_phrasings` (Keywords):** List additional relevant keywords and key phrases (beyond the main sub-query) that should be naturally integrated into the content to capture a wider range of related user intents and improve SEO performance. These should be derived from the analysis of competitive content. Present as a list of strings.\n\n            Ensure the output is a single, valid JSON object that can be directly parsed.\n            \"\"\"\n            analysis_result = call_gemini_api(prompt, cost_tracker=cost_tracker, grounding_url=grounding_url, default_response_mime_type='application/json')\n\n            if analysis_result and 'ideal_content_profile' in analysis_result:\n                item['ideal_content_profile'] = analysis_result['ideal_content_profile']\n                logger.info(f\"Successfully generated competitive profile for '{sub_query}'.\")\n            else:\n                raise ValueError(\"Gemini API response was malformed or missing 'ideal_content_profile'.\")\n\n        except Exception as e:\n            logger.error(f\"An error occurred during competitive analysis for '{sub_query}': {e}\")\n            item['ideal_content_profile'] = {\"error\": str(e)}\n\n    logger.info(\"Stage 3 (Competitive Analysis) completed.\")\n    return stage2_output\n